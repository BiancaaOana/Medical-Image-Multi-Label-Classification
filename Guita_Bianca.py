# -*- coding: utf-8 -*-
"""DeepLearning_(4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lF67wvg0d062uZWWNn6lvQuRS_RGIWum

# **Files upload and unzip**
"""

from google.colab import files

files.upload()

! unzip /content/dl-2022-medical.zip

"""# **Libraries**"""

# Libraries
import numpy as np
import pandas as pd
import os
import csv
import matplotlib.pyplot as plt
import seaborn as sns
import cv2 as cv
from torchvision.transforms.functional import to_tensor, normalize
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms
from pandas.io.formats.style_render import DataFrame
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gc
import torch.optim as optim
import torch.nn as nn
from sklearn.decomposition import PCA                  
from sklearn.svm import SVC                            
from sklearn.pipeline import make_pipeline             
from sklearn.model_selection import train_test_split   
from sklearn.model_selection import GridSearchCV       
from sklearn import metrics 
from torch.utils.data import DataLoader

"""# **Data Processing**"""

train = pd.read_csv("train_labels.csv")
train

train_labels=train.drop(columns='id')

len(train_labels.iloc[:,0:].sum().values)

len(train_labels.columns.values)

sns.set(font_scale = 1.5)
plt.figure(figsize=(11,5))
bxis= sns.barplot(train_labels.columns.values, train_labels.iloc[:,0:].sum().values)
plt.title("Photos in each category", fontsize=12)

a = bxis.patches
labels = train_labels.iloc[:,2:].sum().values
plt.show()

ml= train.iloc[:,0:].sum(axis=1).value_counts()
ml = ml.iloc[1:]
sns.set(font_scale = 2)
plt.figure(figsize=(10,5))
axis2 = sns.barplot(ml.index, ml.values)
plt.title("Photos having multiple labels ")

a = axis2.patches
labels = ml.values

plt.show()

train_labels.columns

train.columns

# Reading validation csv
val_test =  pd.read_csv("val_labels.csv")
val_test

# Reading test csv that actually is the submission one
test =  pd.read_csv("sample_submission.csv")
test

# Making arrays for train data
train_arr = train.values
path = "/content/train_labels.csv"

# Making arrays for validation data
val_arr = val_test.values
path = "/content/val_labels.csv"

# Making arrays for test data
test_arr = test.values
path = "/content/test.csv"

# Class for the dataset
class Image_Set(Dataset):
    def __init__(self, img, lbl):
        self.img = img
        self.lbl = lbl

    def __len__(self):
        return len(self.img)

    def __getitem__(self, idx):
        return self.img[idx], self.lbl[idx]

# Function that gets the images ready to load in loaders
def have_img(dir,names):

    img_names = []
    images = []

    for img_name in names:
        img_names.append(img_name)

    for img_name in img_names:
        img = cv.imread(os.path.join(dir, img_name), 0) 
        img = cv.resize(img,(224,224))
        img = to_tensor(img)    
        img = normalize(img, [0.485], [0.229])
        images.append(img)

    return torch.unsqueeze(torch.cat(images, dim=0), dim=1)

submission = pd.read_csv("sample_submission.csv")

# Loading the images
train_images = have_img("./train_images/",train['id'])
val_images = have_img("./val_images/",val_test['id'])
test_images = have_img("./test_images/",submission['id'])

# Verifying that my images are in good shape
print(train_images.shape)
print(val_images.shape)
print(test_images.shape)

#for lstm
# train_images2=train_images.squeeze(1)
# val_images2=val_images.squeeze(1)
# train_images2.shape

#Providing the labels
train_labels=train.drop(columns='id')
val = pd.read_csv("val_labels.csv")
val_labels=val.drop(columns='id')

# making sure the format of the labels for train and validation is suitable
train_labels = train_labels.values
train_labels = np.float32(np.array(train_labels))
train_labels = to_tensor(train_labels)
train_labels = train_labels.squeeze(0)

val_labels = val_labels.values
val_labels = np.float32(np.array(val_labels))
val_labels = to_tensor(val_labels)
val_labels = val_labels.squeeze(0)

# val_labels2=val_labels.squeeze(0)
# val_labels2.size()

print(train_labels.shape)
print(val_labels.shape)

print(len(test_images))
print(len(train_images))
print(len(val_images))

x_train = train_arr
x_test = test_arr

# Setting a batch size of... and creating dataloaders
batch_size=64
# Imi creez dataloaderele de train, test si validation
train_loader = DataLoader(Image_Set(train_images, train_labels), batch_size, shuffle=True)
validation_loader = DataLoader(Image_Set(val_images, val_labels), batch_size=1, shuffle=True)

# # Setting a batch size of... and creating dataloaders
# batch_size=48
# # Imi creez dataloaderele de train, test si validation
# train_loader = DataLoader(Image_Set(train_images2, train_labels), batch_size, shuffle=True)
# validation_loader = DataLoader(Image_Set(val_images2, val_labels), batch_size, shuffle=True)

val_arr[:,:4]

# Testing the train loader
for x,y in train_loader:
    print(x.shape)
    print(y.shape)
    break

"""# **CNN**"""

class THE_model(nn.Module):

  def __init__(self):
    super(THE_model, self).__init__()
    self.layer1 = nn.Conv2d(in_channels = 1, out_channels=1024, kernel_size = (7,7), stride = (2,2))
    self.layer2 = nn.Conv2d(in_channels=1024, out_channels=3, kernel_size=(5,5))


# Pooling the activation function
    self.avgPool = nn.MaxPool2d(2,2)
    self.activation = nn.ReLU()

# Applying Batch Normalization
    self.norm1 = nn.BatchNorm2d(1024)
    self.norm2 = nn.BatchNorm2d(3)

# Also applying a dense layer
    self.linear = nn.Linear(1875 ,3)



#Redim the data in a 1-dim tensor with flatten 
    self.flatten = nn.Flatten()
# Having multiple functions to try for my model. In the end I chose Sigmoid
    self.sigmoid = nn.Sigmoid()
    self.softmax = nn.Softmax(dim=1)
    self.Hardtanh = nn.Hardtanh(0, 1)



  def forward(self,x):
    output = self.activation(self.norm1(self.avgPool(self.layer1(x))))
    output = self.activation(self.norm2(self.avgPool(self.layer2(output))))
  
    output = self.flatten(output)
    output =self.linear(output)
    output = self.sigmoid(output)

    return output

"""# **VGG-custom**"""

class THE_model(nn.Module):

  def __init__(self):
    super(THE_model, self).__init__()
    self.layer1 = nn.Conv2d(in_channels = 1, out_channels=48, kernel_size = (7,7), stride = (2,2), padding=(1, 1))
    self.layer2 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=(5,5), stride = (2,2), padding=(1, 1))
    self.layer3 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=(3,3), stride = (2,2), padding=(1, 1))
    self.layer4 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=(3,3), stride = (2,2), padding=(1, 1))
    self.layer5 =nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer6 =nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer7 =nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer8 =nn.Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.layer9=nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer10=nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer11=nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer19=nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer20=nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    self.layer21=nn.Conv2d(192, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
# Pooling the activation function
    self.avgPool = nn.MaxPool2d(2,2)
    self.activation = nn.ReLU()

# Applying Batch Normalization
    self.norm1 = nn.BatchNorm2d(48)
    self.norm2 = nn.BatchNorm2d(48)
    self.norm3 = nn.BatchNorm2d(48)
    self.norm4 = nn.BatchNorm2d(96)
    self.norm5 = nn.BatchNorm2d(96)
    self.norm6 = nn.BatchNorm2d(96)
    self.norm7 = nn.BatchNorm2d(96)
    self.norm8 = nn.BatchNorm2d(192)
    self.norm9 = nn.BatchNorm2d(192)
    self.norm10 = nn.BatchNorm2d(192)
    self.norm11= nn.BatchNorm2d(192)
    self.norm19= nn.BatchNorm2d(192)
    self.norm20= nn.BatchNorm2d(192)
    self.norm21= nn.BatchNorm2d(1280)

# Also applying a dense layer
    self.linear = nn.Linear(1280  ,3)



#Redim the data in a 1-dim tensor with flatten 
    self.flatten = nn.Flatten()
# Having multiple functions to try for my model. In the end I chose Sigmoid
    self.sigmoid = nn.Sigmoid()
    self.softmax = nn.Softmax(dim=1)
    self.Hardtanh = nn.Hardtanh(0, 1)



  def forward(self,x):
    output = self.activation(self.norm1(self.layer1(x)))
    output = self.activation(self.norm2(self.layer2(output)))
    output = self.activation(self.norm3(self.layer3(output)))
    output = self.activation(self.norm4(self.layer4(output)))
    output = self.activation(self.norm5(self.layer5(output)))
    output = self.activation(self.norm6(self.layer6(output)))
    output = self.activation(self.norm7(self.layer7(output)))
    output = self.activation(self.norm8(self.layer8(output)))
    output = self.activation(self.norm9(self.layer9(output)))
    output = self.activation(self.norm10(self.layer10(output)))
    output = self.activation(self.norm11(self.layer11(output)))
    output = self.activation(self.norm19(self.layer19(output)))
    output = self.activation(self.norm20(self.avgPool(self.layer20(output))))
    output = self.activation(self.norm21(self.avgPool(self.layer21(output))))
    
    output = self.flatten(output)
    output =self.linear(output)
    output = self.sigmoid(output)

    return output

# Giving the number of epochs
epochs =12

# Defining the network
network = THE_model().cuda()

# Defining the Optimizer
optimizer = optim.Adam(network.parameters(), lr=1e-4)

#After defining the optimizer, after each iteration, the function zero_grad needs to be called. This one makes all the gradients 0
optimizer.zero_grad() 

# Defining loss function
loss_fn = nn.BCELoss()

def train_fn(epochs: int, train_loader: DataLoader, validation_loader: DataLoader, net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):
    # Training on GPU
    use_cuda = True
    CUDA_LAUNCH_BLOCKING=1
    best_acc=0

    # Going through epochs number
    for e in range(epochs):

    # Going through every example in loader
        for images, labels in train_loader:
            if use_cuda:
                images = images.cuda()
                labels = labels.cuda()
                # Applying the neural network on our data
                out = net(images)
                # Aplicam loss function the output and imgs annotations 
                loss = loss_fn(out, labels)
                # Back propagation algorithm
                loss.backward()
                # Applying gradients to networks params
                optimizer.step()
                # ByeBye gradients now
                optimizer.zero_grad()
        
        print("Loss of the epoch {} is {}".format(e, loss.item()))
    
        # Accuracy
        count = len(validation_loader)
        correct = 0
        net.eval()
        with torch.no_grad():
            for val_img, val_lbl in validation_loader:
                if use_cuda:
                    val_lbl = val_lbl.cuda()
                    val_img = val_img.cuda()
                    out_class = torch.round(net(val_img))                

                if torch.all(out_class.eq(val_lbl)):
                    correct += 1

        print("correct={}, count={}".format(correct, count))
        
        print("Accuracy of epoch {} is {:.2f}%".format(e, (correct / count) * 100))
        torch.save(net.state_dict(), "model_"+str(e)+".pth")
    if best_acc <  (correct / count) * 100:
              best_acc =  (correct / count) * 100

torch.cuda.is_available()

train_fn(epochs, train_loader, validation_loader, network, loss_fn, optimizer)

# Collecting CUDA
gc.collect()

"""# **RNN**"""

class lstm(nn.Module):
    def __init__(self):
        super(lstm,self).__init__()
        self.lstm = nn.LSTM(1,100,1,batch_first=True)
        self.linear = nn.Linear(100, 3)
        

    def forward(self,x):
        zerosh = torch.zeros(1, x.size(0), 100).requires_grad_()
        zerosc = torch.zeros(1, x.size(0), 100).requires_grad_()
        out, (n_h, n_c) = self.lstm(x, (zerosh.detach(), zerosc.detach()))
        out = self.linear(out[:, -1, :])
        out = self.lstm(out)

        return out

model = lstm(1, 100, 1, 3)

# Giving the number of epochs
epochs =15

# Defining the network
network = model.cuda()

# Defining the Optimizer
optimizer = optim.Adam(network.parameters(), lr=0.1)

#After defining the optimizer, after each iteration, the function zero_grad needs to be called. This one makes all the gradients 0
optimizer.zero_grad() 

# Defining loss function
loss_fn = nn.BCELoss()

train_fn(epochs, train_loader, validation_loader, network, loss_fn, optimizer)

"""# **PREDICTION**"""

# Loading the best accuracy model
network.load_state_dict(torch.load("/content/model_2.pth"))

# Reading sample submission csv
submission = pd.read_csv("sample_submission.csv")
submission

# Function that returns the predictions for my test images
def prediction(net: nn.Module):

    submission = pd.read_csv("sample_submission.csv")

    test_images = have_img("./test_images/",submission['id'])
    
    img_names = [] 
    for img_name in submission['id']:
      img_names.append(img_name)
    predictions = []
    with torch.no_grad():
        for image in test_images:
            image = image.cuda()
            image=image.unsqueeze(0)
            out = torch.round(net(image))            
            out = out.squeeze(0)
            out = out.detach().cpu().numpy()
            
            predictions.append(out)
    label1=[]
    label2=[]
    label3=[]
    for x in predictions:
      label1.append(x[0].astype(int))
      label2.append(x[1].astype(int))
      label3.append(x[2].astype(int))


    return img_names,label1,label2,label3

image_names,label1,label2,label3 = prediction(network)

# Saving the submission in a data frame
df=DataFrame({
    'id': image_names,
    'label1': label1,
    'label2':label2,
    'label3':label3
})
df.to_csv("74.27-vg%.csv",index=False)

prediction(network)